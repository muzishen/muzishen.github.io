<!DOCTYPE html>

<html>

<head>
	<title>Fei Shen</title>

	<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

	<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js"
		integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n"
		crossorigin="anonymous"></script>

	<style type="text/css">
		@import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");

		body {
			font-family: "Roboto", Helvetica, Arial, sans-serif;
			font-size: 16px;
			line-height: 1.5;
			font-weight: 300;
			background-color: #CDCDCD;
		}

		.content {
			width: 900px;
			padding: 25px 30px;
			margin: 25px auto;
			background-color: #fff;
			box-shadow: 0px 0px 10px #999;
			border-radius: 15px;
		}

		table {
			padding: 5px;
		}

		table.pub_table,
		td.pub_td1,
		td.pub_td2 {
			padding: 8px;
			width: 850px;
			border-collapse: separate;
			border-spacing: 15px;
			margin-top: -5px;
		}

		td.pub_td1 {
			width: 50px;
		}

		td.pub_td1 img {
			height: 120px;
			width: 160px;
		}

		div#container {
			margin-left: auto;
			margin-right: auto;
			width: 820px;
			text-align: left;
			position: relative;
			background-color: #FFF;
		}

		div#DocInfo {
			color: #1367a7;
			height: 158px;
		}

		h4,
		h3,
		h2,
		h1 {
			color: #3B3B3B;
		}

		h2 {
			font-size: 130%;
		}

		p {
			color: #5B5B5B;
			margin-bottom: 50px;
		}

		p.caption {
			color: #9B9B9B;
			text-align: left;
			width: 600px;
		}

		p.caption2 {
			color: #9B9B9B;
			text-align: left;
			width: 800px;
		}

		#header_img {
			position: absolute;
			top: 0px;
			right: 0px;
		}

		a:link,
		a:visited {
			color: #1367a7;
			text-decoration: none;
		}

		#mit_logo {
			position: absolute;
			left: 646px;
			top: 14px;
			width: 200px;
			height: 20px;
		}

		table.pub_table tr {
			outline: thin dotted #666666;
		}

		.papericon {
			border-radius: 8px;
			-moz-box-shadow: 3px 3px 6px #888;
			-webkit-box-shadow: 3px 3px 6px #888;
			box-shadow: 3px 3px 6px #888;
			width: 180px;
			margin-top: 5px;
			margin-left: 5px;
			margin-bottom: 5px;
		}

		.papericon_blank {
			width: 160px;
			margin-top: 5px;
			margin-left: 5px;
			margin-bottom: 5px;
		}

		.media {
			outline: thin dotted #666666;
			margin-bottom: 15px;
			margin-left: 10px;
		}

		.media-body {
			margin-top: 5px;
			padding-left: 20px;
		}

		.papers-selected h5,
		.papers-selected h4 {
			display: none;
		}

		.papers-selected .publication {
			display: none;
		}

		.paperhi-only {
			display: none;
		}

		.papers-selected .paperhi {
			display: flex;
		}

		.papers-selected .paperlo {
			display: none;
		}

		.awards-selected h5,
		.awards-selected h4 {
			display: none;
		}

		.awards-selected .publication {
			display: none;
		}

		.awardhi-only {
			display: none;
		}

		.awards-selected .awardhi {
			display: flex;
		}

		.awards-selected .awardlo {
			display: none;
		}

		.hidden>div {
			display: none;
		}

		.visible>div {
			display: block;
		}
	</style>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag () { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-23931362-2');
	</script>

	<script type="text/javascript">
		var myPix = new Array("img/ai_demo.png")
		function choosePic () {
			var randomNum = Math.floor(Math.random() * myPix.length);
			document.getElementById("myPicture").src = myPix[randomNum];
		};
	</script>

	<script>
		$(document).ready(function () {
			$('.paperlo button').click(function () {
				$('.papers-container').addClass('papers-selected');
			});
			$('.paperhi button').click(function () {
				$('.papers-container').removeClass('papers-selected');
			});

			$('.awardlo button').click(function () {
				$('.awards-container').addClass('awards-selected');
			});
			$('.awardhi button').click(function () {
				$('.awards-container').removeClass('awards-selected');
			});

			$('.text_container').addClass("hidden");

			$('.text_container').click(function () {
				var $this = $(this);

				if ($this.hasClass("hidden")) {
					$(this).removeClass("hidden").addClass("visible");
					$(this).removeClass("papericon");
				} else {
					$(this).removeClass("visible").addClass("hidden");
				}
			});


		});
	</script>

</head>


<body>
	<div class="content">
		<div id="container">

			<table>
				<tbody>
					<tr>
						<td><img id="myPicture" src="xxx" style="float:left; padding-right:20px; height:250px; width:200px;" /></td>
						<script>choosePic();</script>
						<td>
							<div id="DocInfo">
								<h1>Fei Shen (æ²ˆé£)</h1>
								Postdoc<br>
								National University of Singapore <br>
								School of Computing<br>
								NExT++ Research Center<br>
								Email: feishen_at_njust.edu.cn<br>
								<!-- <a href="TH_CV.pdf" target="_blank" rel="external">CV</a> &bull; -->
								<a href="https://scholar.google.com/citations?user=wqvr28MAAAAJ&hl=en" target="_blank"
									rel="external">Google Scholar</a> &bull; <a href="https://github.com/muzishen" target="_blank"
									rel="external">Github</a><br>
							</div><br>
						</td>
					</tr>
				</tbody>
			</table>
			<br>

			<h2>About Me</h2>
			<!-- <p style="text-align:justify" ;> -->
				<p style="text-align: justify;"> Hi ğŸ˜„! I am currently a Postdoctoral Research Fellow at the <a href="https://www.nextcenter.org/">NExT++ Center</a>, National University of Singapore (NUS). I actively collaborate with leading industry partners such as <a href="https://www.guiji.ai/">GuijiAI</a>, <a href="https://aiuni.ai/">AIUNI</a>, <a href="https://www.chumenwenwen.com/zh/">Mobvoi</a>, <a href="http://dev3.noahlab.com.hk/">Huawei</a>, and <a href="https://ailab.tencent.com/">Tencent AI Lab</a>, focusing on cutting-edge research in 2D/3D content generation.
					My research interests center around <span style="color: red; font-weight: bold;">human-centered, controllable, and consistent visual generation</span>, including <em>pose-guided person generation</em>, <em>virtual try-on</em>, and <em>long-term talking face synthesis</em>.
					
					Driven by the vision of accelerating progress toward digital immortality and machine consciousness, I remain committed to pushing the boundaries of generative AI. Prior to my current role, I was an avid algorithm competition enthusiast, earning over 50 top-three finishesâ€”including five championships in CCF A-class workshop challenges.
					
					I am a strong advocate of <span style="text-decoration: underline; font-weight: bold; color: rgb(128, 0, 128);">sharingğŸŒ±, collaboratingğŸ¤, advancingğŸš€, and innovatingğŸ’¡</span>. Passionate about bridging fundamental research and real-world applications, I strive to ensure that my work contributes to both academic advancement and societal impact.
					
					<span style="color: red; font-weight: bold;"><em>If youâ€™re interested in collaborating or would like to connect, feel free to reach out via email!</em></span> ğŸ˜Š
					
					</p>
				  

			<h2><span style="color:red;font-size:27px"><strong>NEWS!</strong></span></h2>
			<ul style="height: 200px;overflow-y: auto">
				<div style="text-align: justify; display: block; margin-right: auto;">
					<li>2025/05: Two papers ware accepted by ICML 2025.</li>
					<li>2025/04: New Task! We released <a
						href="https://github.com/muzishen/IMAGGarment-1"><u>IMAGGarment-1</u></a> for fine-grained garment generation.</li>
					<li>2025/01: Two papers ware accepted by ICLR 2025.</li>
					<li>2024/12: Five papers ware accepted by AAAI 2025.</li>
					<li>2024/11: We released <a href="https://github.com/muzishen/IMAGPose"><u>IMAGPose</u></a>, a unified conditional framework for pose-guided person generation.</li>
					<li>2024/09: One paper was accepted by NeurIPS 2024.</li>
					<li>2024/08: We released <a
						href="https://github.com/muzishen/RCDMs"><u>RCDMs</u></a> for story visualization.</li>
					<li>2024/05: We released <a
							href="https://imagdressing.github.io/"><u>IMAGDressing-v1</u></a> for customizable virtual dressing.</li>
					<li>2024/05: We released <a
							href="https://tenvence.github.io/p/v-express/"><u>V-Express</u></a> for portrait video generation.</li>
					<li>2024/02: We released <a
							href="https://github.com/tencent-ailab/PCDMs"><u>PCDMs</u></a> for pose-guided person synthesis.</li>					
					<li>2024/01: One paper was accepted by ICLR 2024.</li>
					<li>2023/08: We won the <a href="./img/ijcai2023.pdf"><u>2nd place</u></a> in Rotated Detection Challenge organized by IJCAI 2023 workshop.</li>
					<li>2023/07: One paper was accepted by by ACM Multmedia 2023.</li>
					<!--<li>2023/04: I was selected for the Rhino-Bird Elite Talent Program by Tencent.</li>-->
					<li>2023/03: We won the <a href="./img/2022_1_nsfc.pdf"><u>1st place</u></a> of Remote Sensing Image
						Segmentation Contest organized by NSFC.</li>
					<li>2023/01: One paper was accepted by by IEEE T-IP (JCR Q1, IF=11.04).</li>
					<!-- <li>2022/10: We again won <a href="./img/2022_digix_2nd.pdf"><u>2nd place</u></a> in the Global Campus AI
						Algorithms Challenge organized by HuaWei.</li>
					<li>2022/10: I was selected as the publicity ambassador of the HDC</li>
					<li>2022/05: We won the <a href="./img/2022_cvpr_3rd.jpg"><u>3rd place</u></a> in pet biometric Challenge
						organized by CVPR 2022 workshop.</li>
					<li> 2022/05: We won the <a href="./img/2022_icme_2nd.jpg"><u>2nd place</u></a> in few-shot logo detection
						challenge organized by ICME2022 workshop.</li>
					<li>2022/03: One paper was accepted by ICME2022 (Oral).</li> -->
					<!--<li>2021/12: ğŸ¥‡ Celebrating a Milestone. My Accumulated Competition Prize Winnings Reach 1,000,000 RMB.</li>-->					
					<!-- <li>2021/11: One paper was accepted by BMVC.</li>
					<li> 2021/09: One paper was accepted by IEEE IOTJ.</li>
					<li>2021/06: One paper was accepted by IEEE T-ITS.</li>
					<li>2020/05: We won the <a href="./img/2020_eccv_1st.pdf"><u>1st place</u></a> in COCO Detection Challenge
						organized by ECCV 2020 workshop.</li> -->
				</div>
			</ul>

			<h2>Research Experience</h2>

			<div>
				&emsp; <strong>National University of Singapore, Singapore (Apr 2025 - Present)</strong>
				<img border="0" src="img/NEXT++.png" align="right" width="120" height="60" />
				<ul>
					<li>
						Postdoctoral Research Fellow, <a href="https://www.nextcenter.org/" target="_blank" rel="external">NExT++</a>
					</li>
					<li>
						Leader: Prof. <a href="https://www.chuatatseng.com/" target="_blank"
							rel="external">Chua Tat-Seng</a>
				
				</ul>
				</li>
				</ul>
			</div>

			<div>
				&emsp; <strong>National University of Singapore, Singapore (May 2024 - Mar 2025)</strong>
				<img border="0" src="img/NEXT++.png" align="right" width="120" height="60" />
				<ul>
					<li>
						Visiting Research Scholar, <a href="https://www.nextcenter.org/" target="_blank" rel="external">NExT++</a>
					</li>
					<li>
						Leader: Prof. <a href="https://www.chuatatseng.com/" target="_blank"
							rel="external">Chua Tat-Seng</a>
				
				</ul>
				</li>
				</ul>
			</div>
			
			
			<div>
				&emsp; <strong>Tencent Inc., Shenzhen China (May 2023 - May 2024)</strong>
				<!-- <a href="https://www.smu.edu.sg/" target="_blank" rel="external"> -->
				<img border="0" src="img/ailab.jpg" align="right" width="120" height="60" />
				<!-- </a> -->
				<ul>
					<li>
						Intern, <a href="https://ai.tencent.com" target="_blank" rel="external">AI Lab</a> (IP-Adapter Team)
					</li>
					<li>
						Leader: Dr. <a href="https://scholar.google.com/citations?user=XGVV3gEAAAAJ&hl=zh-CN&oi=ao" target="_blank"
							rel="external">Xiao Han</a>
					<li>
						Mentor: Dr. <a href="https://scholar.google.com.hk/citations?user=xUMuDgwAAAAJ&hl=zh-CN" target="_blank" rel="external">Jun Zhang</a> 
						and <a href="https://scholar.google.com/citations?hl=zh-TW&user=Jh_weSQAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="external">Hu Ye</a> 
					</li>
				</ul>
				</li>
				</ul>
			</div>

			<div>
				&emsp; <strong>Nanjing University of Science and Technology, Nanjing China </strong>
				<!-- <a href="https://www.sutd.edu.sg/" target="_blank" rel="external"> -->
				<img border="0" src="img/imag.png" align="right" width="120" height="100" />
				<!-- </a> -->
				<ul>
					<li>
						PhD, <a href="https://imag-njust.net/" target="_blank" rel="external">IMAG</a>
					</li>
					<li>
						Supervised by Prof. <a href="https://scholar.google.com/citations?user=ByBLlEwAAAAJ&hl=zh-CN" target="_blank"
							rel="external">Jinhui
							Tang</a>
						</li>
					</li>
				</ul>
			</div>




			<!-- <h2>Research Interests</h2>

			<ul>
				<h6><strong style="color:#ff0000">Learning From Limited or Imperfect Data</strong></h6>
				<li> Multimedia: Zero/Few-shot Learning, Fine-Grained Visual Classification/Retrieval, ... </li>
				<li> Computer Vision: Multi-Modality Object Localization/Detection/Segmentation </li>
			</ul> -->

	

			<div class="papers-container">
				<h5 class="paperhi">Selected Publications</h5>
			  
				<div class="publication media paperhi">
				  <div class="media-body">
				  <strong>IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion Design</strong><br>
				  <strong>F. Shen</strong>, J. Yu, C. Wang, X. Jiang, X. Du, J. Tang<br>
				  Under Review
				  [<a href="https://arxiv.org/pdf/2504.13176" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/IMAGGarment-1">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>New task! Controllable fashion design.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</strong><br>
				  <strong>F. Shen</strong>, C. Wang, J. Gao, Q. Guo, J. Dang, J. Tang, T.-S. Chua<br>
				  ICML 2025
				  [<a href="https://arxiv.org/pdf/2502.09533" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/MCDM/">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Proposed multilingual long-term talkingface.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>FaceShot: Bring Any Character into Life</strong><br>
				  J. Gao, Y. Sun, <strong>F. Shen</strong>, X. Jiang, Z. Xing, K. Chen, C. Zhao<br>
				  ICLR 2025
				  [<a href="https://arxiv.org/pdf/2407.01414" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/open-mmlab/FaceShot/">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Talkingface plugin for any character.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>Ensembling Diffusion Models via Adaptive Feature Aggregation</strong><br>
				  C. Wang, K. Tian, Y. Guan, <strong>F. Shen</strong>, Z. Jiang, Q. Gu, J. Zhang<br>
				  ICLR 2025
				  [<a href="https://arxiv.org/pdf/2405.17082" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/tenvence/afa/">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Uses routing aggregation for ensembling multiple diffusion models.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>IMAGDressing-v1: Customizable Virtual Dressing</strong><br>
				  <strong>F. Shen</strong>, X. Jiang, X. He, H. Ye, C. Wang, X. Du, Z. Li, J. Tang<br>
				  AAAI 2025
				  [<a href="https://arxiv.org/pdf/2407.12705" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/IMAGDressing">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Proposes the virtual dressing task and introduces a new dataset.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>Boosting Consistency in Story Visualization with Rich-Contextual Conditional Diffusion Models</strong><br>
				  <strong>F. Shen</strong>, H. Ye, S. Liu, J. Zhang, C. Wang, X. Han, W. Yang<br>
				  AAAI 2025
				  [<a href="https://arxiv.org/pdf/2407.02482" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/RCDMs">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Proposes non-autoregressive diffusion models for story generation.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation</strong><br>
				  C. Wang, K. Tian, J. Zhang, Y. Guan, F. Luo, <strong>F. Shen</strong>, Z. Jiang, Q. Gu, X. Han, W. Yang<br>
				  Under Review
				  [<a href="https://arxiv.org/pdf/2406.02511" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/tencent-ailab/V-Express/">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Explores strong and weak conditional relationships for portrait video generation.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>IMAGPose: A Unified Conditional Framework for Pose-Guided Person Generation</strong><br>
				  <strong>F. Shen</strong>, J. Tang<br>
				  NeurIPS 2024
				  [<a href="https://openreview.net/pdf?id=6IyYa4gETN" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/IMAGPose">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Unifies person generation under diverse conditions.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models</strong><br>
				  <strong>F. Shen</strong>, H. Ye, J. Zhang, C. Wang, X. Han, W. Yang<br>
				  ICLR 2024
				  [<a href="https://arxiv.org/pdf/2310.06313.pdf" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/tencent-ailab/PCDMs">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Introduces concat/inpainting methods for generation.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>GiT: Graph Interactive Transformer for Vehicle Re-identification</strong><br>
				  <strong>F. Shen</strong>, Y. Xie, J. Zhu, X. Zhu, H. Zeng<br>
				  IEEE Transactions on Image Processing
				  [<a href="https://ieeexplore.ieee.org/document/10026500" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/ReID-GiT">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Graph reasoning with interaction-aware transformer for re-ID.</strong></span>
				</div>
			  </div>
			
			  <div class="publication media paperhi">
				<div class="media-body">
				  <strong>Pedestrian-specific Bipartite-aware Similarity Learning for Text-based Person Retrieval</strong><br>
				  <strong>F. Shen</strong>, X. Shu, X. Du, J. Tang<br>
				  ACM Multimedia 2023
				  [<a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612009" target="_blank" rel="external">PDF</a>]
				  [<a href="https://github.com/muzishen/PBSL">Code</a>]<br>
				  <strong style="color:red;">TL;DR:</strong> <span style="color:red;"><strong>Adapts bipartite structure for text-to-person retrieval.</strong></span>
				</div>
			  </div>
			</div>
			


			<!--<sup>&#x2709</sup>-->
			<div class="awards-container awards-selected">
				<h5 class="awardlo">All Awards<button type="button" class="ml-3 btn btn-light">Show selected</button>
				</h5>
				<h5 class="awardhi awardhi-only">Selected Awards<button type="button" class="ml-3 btn btn-light">Show
						all</button></h5>

				<h5 class="pt-2 pb-1">2023 </h5>

				<!-- <div class="publication media">
					<div class="media-body">
						Object Re-identification Using Teacher-Like and Light Students</strong></a><br>
						Yi Xie, Hanxiao Wu, Fei Shen, Jianqing Zhu, and Huanqiang Zeng <br>
						BMVC 2021 
						[<a href="https://www.bmvc2021-virtualconference.com/assets/papers/1193.pdf" target="_blank" rel="external"><strong style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/EMRN">Code</a>]<br>
					</div>
				</div> -->
				
				<div class="publication media awardhi">
					<div class="media-body">
						IJCAI 2023 Workshop Challenge, Track IIï¼šRotated Detection, Runner-up </a><br>
						<!-- Zijun Huang and Fei Shen <br> -->
						Organizer: IJCAI <br>
					</div>
				</div>
				
				<div class="publication media awardhi">
					<div class="media-body">
						2022-2023â€œèˆªå¤©å®å›¾æ¯â€é¥æ„Ÿå½±åƒæ™ºèƒ½å¤„ç†ç®—æ³•å¤§èµ›-é¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²,ä¸€ç­‰å¥–</a><br>
						<!-- é»„æ¢“é’§, ç‹æ™¨æ˜Š, ä½•å…¸, æè‹±é¾™, æ²ˆé£<br> -->
						ä¸»åŠå•ä½ï¼šå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å§”å‘˜ä¼šä¿¡æ¯ç§‘å­¦éƒ¨ <br>
					</div>
				</div>
				
				<div class="publication media">
					<div class="media-body">
						2023å›½é™…å¤§æ•°æ®äº§ä¸šåšè§ˆä¼š-ç”Ÿæ´»åƒåœ¾æ£€æµ‹,äºŒç­‰å¥–</a><br>
						<!-- æ±Ÿèˆ’, æˆ¿æ¢“æ™”, ä»˜å­å¾·, æ²ˆé£  <br> -->
						ä¸»åŠå•ä½ï¼šè´µé˜³å¤§æ•°æ®äº¤æ˜“æ‰€ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2023IEEE IGRSS Data Fusion Contest Track 1, 3rd place</a><br>
						<!-- Jiarui Hu, Zijun Huang, and Fei Shen<br> -->
						Organizer: IEEE IGRSS<br>
					</div>
				</div>

				<h5 class="pt-2 pb-1">2022 </h5>



				<div class="publication media">
					<div class="media-body">
						2022ç§‘å¤§è®¯é£1024æŒ‘æˆ˜èµ›,2ä¸ªå† å†›;1ä¸ªäºšå†›;1ä¸ªå­£å†›</a><br>
						<!-- æ²ˆé£, ä»»ä¿Šå¼›, è’‹é‘«, ä»˜å­å¾·, æ±Ÿèˆ’, æˆ¿æ¢“æ™”, é­æ¢¦å©‰<br> -->
						åŸºäºå¯è§å…‰å›¾åƒçš„æŸ‘æ©˜èŠ±æœæ¢¢è¯†åˆ«æŒ‘æˆ˜èµ›, ç¬¬ä¸€å<br>
						è¾£æ¤’ç—…è™«å®³å›¾åƒè¯†åˆ«æŒ‘æˆ˜èµ›, ç¬¬ä¸€å<br>
						æ™ºèƒ½ç¡¬ä»¶è¯­éŸ³æ§åˆ¶çš„æ—¶é¢‘å›¾åˆ†ç±»æŒ‘æˆ˜èµ› 2.0, ç¬¬äºŒå<br>
						åŸºäºå°æ ·æœ¬çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡æŒ‘æˆ˜èµ›, ç¬¬ä¸‰å<br>
						ä¸»åŠå•ä½ï¼šç§‘å¤§è®¯é£ <br>
			
					</div>
				</div>



				<div class="publication media">
					<div class="media-body">
						2022 å…¨å›½å¤§å­¦ç”Ÿç‰©è”ç½‘è®¾è®¡ç«èµ›,ä¸€ç­‰å¥–</a><br>
						<!-- æ²ˆé£, è’‹é‘«, ä»˜å­å¾·, é™ˆå˜‰ç…œ<br> -->
						ä¸»åŠå•ä½ï¼šå…¨å›½é«˜ç­‰å­¦æ ¡è®¡ç®—æœºæ•™è‚²ç ”ç©¶ä¼š <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2022 kaggle å¾®ç”Ÿç‰©ç›®æ ‡æ£€æµ‹,ç¬¬ä¸€å</a><br>
						ä¸»åŠå•ä½ï¼škaggle <br>
						<!-- æ²ˆé£<br> -->
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2022åä¸ºdigixå…¨çƒæ ¡å›­ç²¾è‹±æŒ‘æˆ˜èµ›-è½¦é“çº¿æ¸²æŸ“æ£€æµ‹,<a href="img/2022_digix_2nd.pdf">äºšå†›, [<a href="https://github.com/muzishen/Pet-ReID-IMAG">Code</a>]</a><br>
						<!-- æ²ˆé£, è’‹é‘«, é™ˆå˜‰ç…œ [<a href="https://github.com/muzishen/Pet-ReID-IMAG">Code</a>] <br> -->
						ä¸»åŠå•ä½ï¼šæ±Ÿè‹çœäººå·¥æ™ºèƒ½å­¦ä¼š, åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2022 few-shot logo detection challenge of ICME workshop, 2nd place</a><br>
						<!-- Zhe Wang, Baoying Chen, and Fei Shen<br> -->
						Organizer: IEEE ICME <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2022é¦–å±Šâ€œå…´æ™ºæ¯â€å…¨å›½äººå·¥æ™ºèƒ½åˆ›æ–°åº”ç”¨å¤§èµ›, ä¸‰ç­‰å¥–</a><br>
						<!-- æ²ˆé£, è’‹é‘«, ç‹æ™‹, ä½•æ¨å‡Œ<br> -->
						ä¸»åŠå•ä½ï¼šå…¨å›½äººå·¥æ™ºèƒ½å¤§ä¼š <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2022èˆ¹æµ·æ•°æ®åº”ç”¨åˆ›æ–°å¤§èµ›-æµ·ä¸Šç›®æ ‡æ£€æµ‹, ä¸‰ç­‰å¥–</a><br>
						<!-- æ²ˆé£, é»„æ¢“é’§<br> -->
						ä¸»åŠå•ä½ï¼šä¸­å›½èˆ¹èˆ¶é›†å›¢, æ·±æµ·ç§‘å­¦æŠ€æœ¯å¤ªæ¹–å®éªŒå®¤ <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2022 IEEE UV â€œVision Meets Algaeâ€ Object Detection Challenge, 3rd
							place</a><br>
						<!-- Xiaode Fu and Fei Shen<br> -->
						Organizer: IEEE UV <br>
					</div>
				</div>



				<div class="publication media awardhi">
					<div class="media-body">
						2022 pet biometric Challenge of CVPR workshop, 3rd place, [<a href="https://github.com/muzishen/Pet-ReID-IMAG">Code</a>]</a><br>
						<!-- Fei Shen, Zhe Wang, Zijun Huang, Xiaode Fu, and Jiayi Chen<br> -->
						<!-- [<a href="https://github.com/muzishen/Pet-ReID-IMAG">Code</a>] -->
						Organizerï¼šCVPR <br>
					</div>
				</div>



				<h5 class="pt-2 pb-1">2021 </h5>

				<div class="publication media">
					<div class="media-body">
						2021åä¸ºäº‘â€œä¸œå´æ¯â€æ•°å­—è½¬å‹åˆ›æ–°å¤§èµ›-æ±½è½¦é›¶éƒ¨ä»¶æ£€æµ‹,ä¸€ç­‰å¥–</a><br>
						<!-- æ²ˆé£ <br> -->
						ä¸»åŠå•ä½ï¼šè‹å·å¸‚æ”¿åºœ, åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2021æµ·æ´‹ç›®æ ‡æ™ºèƒ½æ„ŸçŸ¥å›½é™…æŒ‘æˆ˜èµ›-å¯è§å…‰ç›®æ ‡æ£€æµ‹,ä¸€ç­‰å¥–</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, å´å«ç¬‘<br> -->
						ä¸»åŠå•ä½ï¼šä¸­å›½é€ èˆ¹å·¥ç¨‹å­¦ä¼š, å›½é™…èˆ¹èˆ¶ä¸æµ·æ´‹å·¥ç¨‹åˆ›æ–°ä¸åˆä½œè”ç›Ÿ(ICNAME), ä¸­å›½å›¾è±¡å›¾å½¢å­¦å­¦ä¼š<br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2021ç¦å»ºçœè½¯ä»¶è®¾è®¡å¤§èµ›,ä¸€ç­‰å¥–</a><br>
						<!-- æ²ˆé£<br> -->
						ä¸»åŠå•ä½ï¼šç¦å»ºçœå·¥ä¸šå’Œä¿¡æ¯åŒ–å… <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2021ç§‘å¤§è®¯é£1024æŒ‘æˆ˜èµ›,3ä¸ªå† å†›;1ä¸ªäºšå†›</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, ä½•æ–°, ä½•ä¼¯å‹‡<br> -->
						æŸ‘æ©˜ç—…è™«å®³è¯†åˆ«æŒ‘æˆ˜èµ›, ç¬¬ä¸€å<br>
						é¥æ„Ÿå½±åƒå…¸å‹ç›®æ ‡æå–æŒ‘æˆ˜èµ›, ç¬¬ä¸€å<br>
						å†œä½œç‰©ç”Ÿé•¿æƒ…å†µè¯†åˆ«æŒ‘æˆ˜èµ›, ç¬¬ä¸€å<br>
						å¶èœç—…è™«å®³å›¾åƒè¯†åˆ«æŒ‘æˆ˜èµ›, ç¬¬äºŒå<br>
						ä¸»åŠå•ä½ï¼šç§‘å¤§è®¯é£ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2021å…¨å›½é«˜æ ¡æ–°æ˜ŸæŒ‘æˆ˜èµ›-ä¼ä¸šå‡ºé¢˜,æ­æ¦œæŒ‚å¸…, åŒèµ›é“ä¸€ç­‰å¥–</a><br>
						<!-- æ²ˆé£<br> -->
						æ˜Ÿå®¸ç§‘æŠ€è½»é‡åŒ–äººè„¸æ£€æµ‹ç¬¬ä¸€å<br>
						äº¿è”ç§‘æŠ€å®¤å†…è¡Œäººè·Ÿè¸ªç¬¬ä¸€å<br>
						ä¸»åŠå•ä½ï¼šå¦é—¨å·¥ä¸šå’Œä¿¡æ¯åŒ–å±€ <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2021â€œæ˜“åå½•â€æ¯å›¾åƒæè¿°ç”Ÿæˆ, ç¬¬äºŒå</a><br>
						<!-- ä½•æ–°, æ²ˆé£<br> -->
						ä¸»åŠå•ä½ï¼šå¤©æ´¥å·¥ä¸šå’Œä¿¡æ¯åŒ–å±€, åŒ—äº¬æ˜“åå½•ä¿¡æ¯æŠ€æœ¯è‚¡ä»½æœ‰é™å…¬å¸ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2021ä¸­å›½è½¯ä»¶æ¯-è½¯ä»¶è®¾è®¡å¤§èµ›, äºŒç­‰å¥–, [<a href="https://github.com/muzishen/China-Software-Cup">Code</a>]</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, å´å«ç¬‘ [<a href="https://github.com/muzishen/China-Software-Cup">Code</a>]<br> -->
						ä¸»åŠå•ä½ï¼šå·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨ <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2021å…¨å›½äººå·¥æ™ºèƒ½åˆ›æ–°(AIAC)åº”ç”¨å¤§èµ›-é’¢é“ç‘•ç–µæ£€æµ‹, äºŒç­‰å¥–</a><br>
						<!-- æ²ˆé£<br> -->
						ä¸»åŠå•ä½ï¼šä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼š <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2021å…¨å›½æ•°å­—ç”Ÿæ€åˆ›æ–°å¤§èµ›-æ™ºèƒ½ç®—æ³•èµ›, ç¬¬ä¸‰å</a><br>
						<!-- ä½•æ–°, æ²ˆé£, éƒå…ƒæ´<br> -->
						ä¸»åŠå•ä½ï¼šå¤©æ±  <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						WAICä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼š-ç®—æ³•æ‰©å±•æŒ‘æˆ˜èµ›, å­£å†›</a><br>
						<!-- æ²ˆé£<br> -->
						ä¸»åŠå•ä½: WAIC <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2021â€œæ˜‡è…¾æ¯â€é¥æ„Ÿå½±åƒæ™ºèƒ½å¤„ç†ç®—æ³•å¤§èµ›-ç»†ç²’åº¦è¯­ä¹‰åˆ†å‰², ä¸‰ç­‰å¥–</a><br>
						<!-- èµµæ…§ç³, å¦èª, ä½•æ¬£, æ²ˆé£<br> -->
						ä¸»åŠå•ä½ï¼šå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å§”å‘˜ä¼šä¿¡æ¯ç§‘å­¦éƒ¨ <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2021æµ·æ´‹ç›®æ ‡æ™ºèƒ½æ„ŸçŸ¥å›½é™…æŒ‘æˆ˜èµ›-çº¢å¤–ç›®æ ‡æ£€æµ‹, ä¸‰ç­‰å¥–</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, å´å«ç¬‘<br> -->
						ä¸»åŠå•ä½ï¼šä¸­å›½é€ èˆ¹å·¥ç¨‹å­¦ä¼š, å›½é™…èˆ¹èˆ¶ä¸æµ·æ´‹å·¥ç¨‹åˆ›æ–°ä¸åˆä½œè”ç›Ÿ(ICNAME), ä¸­å›½å›¾è±¡å›¾å½¢å­¦å­¦ä¼š <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2022 å…¨å›½æ°´ä¸‹å…‰å­¦ç›®æ ‡æ£€æµ‹æ™ºèƒ½ç®—æ³•èµ›, å…¥å›´å¥–</a><br>
						<!-- ä½•æ–°ï¼Œæ²ˆé£, æè§£<br> -->
						ä¸»åŠå•ä½ï¼šå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å§”å‘˜ä¼šä¿¡æ¯ç§‘å­¦éƒ¨ <br>
					</div>
				</div>

				<h5 class="pt-2 pb-1">2020</h5>


				<div class="publication media awardhi">
					<div class="media-body">
						2020 Visual Inductive Priors for Data-Efficient Computer Vision Object Detection, 1st place, [<a href="https://github.com/muzishen/VIPriors-Object-Detection-Challenge">Code</a>]</a><br>
						<!-- Fei Shen [<a href="https://github.com/muzishen/VIPriors-Object-Detection-Challenge">Code</a>]<br> -->
						Organizer: ECCV <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2020ä¸­å›½è½¯ä»¶æ¯-è½¯ä»¶è®¾è®¡å¤§èµ›, ä¸€ç­‰å¥–, [<a href="https://github.com/muzishen/China-Software-Cup">Code</a>]</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, é­æ¢¦å©‰ [<a href="https://github.com/muzishen/China-Software-Cup">Code</a>]<br> -->
						ä¸»åŠå•ä½ï¼šå·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2020 æ˜“åå½•ä¸å®šå‘ç®—æ³•èµ›, äºŒç­‰å¥–</a><br>
						<!-- æ²ˆé£, ä½•æ–°, æ—å•¸æ¶›<br> -->
						ä¸»åŠå•ä½ï¼šå¤©æ´¥å·¥ä¸šå’Œä¿¡æ¯åŒ–å±€, åŒ—äº¬æ˜“åå½•ä¿¡æ¯æŠ€æœ¯è‚¡ä»½æœ‰é™å…¬å¸ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2020 åä¸ºdigixå…¨çƒæ ¡å›­ç²¾è‹±æŒ‘æˆ˜èµ›-æ•°ç æ£€ç´¢,ç¬¬å››åï¼Œ[<a href="https://github.com/muzishen/Huawei_Digix_Retrieval_Top4">Code</a>]</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, ä½•æ–° [<a href="https://github.com/muzishen/Huawei_Digix_Retrieval_Top4">Code</a>]<br> -->
						ä¸»åŠå•ä½ï¼šæ±Ÿè‹çœäººå·¥æ™ºèƒ½å­¦ä¼š, åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2020â€œä¸­å…´æ§æœˆâ€ç®—æ³•å¤§èµ›-å¤šç›®æ ‡è·Ÿè¸ª, ä¼˜èƒœå¥–, [<a href="https://github.com/muzishen/Deepsort_V2">Code</a>]</a><br>
						<!-- æ²ˆé£ [<a href="https://github.com/muzishen/Deepsort_V2">Code</a>] <br> -->
						ä¸»åŠå•ä½ï¼šä¸­å…´é€šè®¯è‚¡ä»½æœ‰é™å…¬å¸ <br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						2020 â€œåä¸ºæ¯â€ç¬¬äºŒå±Šä¸­å›½ç ”ç©¶ç”Ÿäººå·¥æ™ºèƒ½åˆ›æ–°åº”ç”¨å¤§èµ›-å°æ ·æœ¬åˆ†ç±», ä¸‰ç­‰å¥–</a><br>
						<!-- æ²ˆé£, è°¢æ‡¿, ä½•æ–°<br> -->
						ä¸»åŠå•ä½ï¼šä¸­å›½å­¦ä½ä¸ç ”ç©¶ç”Ÿæ•™è‚²åä¼š <br>
					</div>
				</div>
			</div>




<h2>Honors</h2>
<!-- <div>
    <ul>
		<li>å—äº¬ç†å·¥å¤§å­¦ä¼˜ç§€åšå£«ç”ŸåŸ¹å…»è®¡åˆ’, å—äº¬ç†å·¥å¤§å­¦, 2024</li>
		<li>Outstanding Scholarship of 2024 Tencent Rhino-bird Research Elite Program, Tencent, 2024</li>
		<li>æ±Ÿè‹çœä¸‰å¥½å­¦ç”Ÿ, æ±Ÿè‹çœæ•™è‚²å…, 2024</li>
        <li>æ ¡é•¿å¥–ç« , å—äº¬ç†å·¥å¤§å­¦, 2023</li>
        <li>ç ”ç©¶ç”Ÿå›½å®¶å¥–å­¦é‡‘, ä¸­å›½æ•™è‚²éƒ¨, 2023</li>
        <li>å…¥é€‰å›½å®¶å…¬æ´¾å‡ºå›½ç•™å­¦é¡¹ç›®, å›½å®¶ç•™å­¦åŸºé‡‘ç®¡ç†å§”å‘˜ä¼š, 2023</li>
        <li>å…¥é€‰åä¸ºå¼€å‘è€…ç”Ÿæ€å‘å±•ç‰¹æ®Šè´¡çŒ®å¥–, åä¸ºå¼€å‘è€…å¤§ä¼š, 2023</li>
        <li>å…¥é€‰è…¾è®¯çŠ€ç‰›é¸Ÿç²¾è‹±äººæ‰è®¡åˆ’, è…¾è®¯, 2023</li>
		<li>Kaggle Master, Kaggle, 2022</li>
        <li>First Prize Scholarship of Nanjing University of Science and Technology, 2021, 2022, 2023</li>
        <li>ç ”ç©¶ç”Ÿå›½å®¶å¥–å­¦é‡‘, ä¸­å›½æ•™è‚²éƒ¨, 2021</li>
        <li>First-class Innovation Scholarship, Ministry of Industry and Information Technology of China, 2017</li>
    </ul>
</div> -->
<div>
    <ul>
		<li> Youth Talents Support Project, Doctoral Student Special Program (é¦–å±Šä¸­å›½ç§‘åé’å¹´äººæ‰æ‰˜ä¸¾-åšå£«ç”Ÿä¸“é¡¹)</li>
        <li>Outstanding Scholarship of the 2024 Tencent Rhino-bird Research Elite Program, Tencent, 2024</li>
        <li>President's Medal, Nanjing University of Science and Technology, 2023</li>
        <li>Graduate National Scholarship, Ministry of Education of China, 2021, 2023</li>
        <li>Recipient of the National Scholarship for Study Abroad Program, China Scholarship Council, 2023</li>
        <li>Huawei Developer Ecosystem Special Contribution Award, Huawei Developer Conference, 2023</li>
        <li>Selected for Tencent Rhino-bird Elite Talent Program, Tencent, 2023</li>
        <li>Kaggle Master, Kaggle, 2022</li>
    </ul>
</div>

<h2>Professional Services</h2>
<div>
    <ul>
        <li>
            <b>Journal Reviewer</b>: <br>
            &emsp; â€¢ T-PAMI | T-IP | T-ITS | T-MM | T-VT | T-NNLS | T-AI | T-CSVT | IOTJ | NCA | NC <br>
        </li>
        <li>
            <b>Conference Reviewer / Program Committee Member</b>: <br>
            &emsp; â€¢ NIPS | ICLR | ICML | CVPR | ECCV | ICCV | IJCAI | MICCAI | ACM MM | ICME <br>
        </li>
    </ul>
</div>

<h2>Others</h2>
<div>
    <ul>
       In my free time, I enjoy activities such as running, playing soccer, and basketball. Additionally, I have undergone brief professional training in table tennis.</li>
    </ul>
</div>


</body>

</html>
